{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4R9luvu0b9w"
      },
      "source": [
        "##**Breast Cancer Detection via Convolutional Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Abstract**"
      ],
      "metadata": {
        "id": "OmPzPaXG1VQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN model is designed with a stack of three convolutional layers and two fully connected layers. Each convolutional layer strategically employs max-pooling and batch normalization, enhancing the network's ability to discern hierarchical features in breast ultrasound images. This architecture is purpose-built to classify these images into the categories of benign or malignant. The normal class has been excluded from the model, as the image composition is vastly different from the other classes, causing the convolution errors in running. The utilization of binary crossentropy loss and accuracy metrics during model compilation underscores its' suitability for multiclass image classification tasks. The comprehensive summary of the CNN model provides an overview of its structure and layer configurations, conveying the intricate architecture designed for optimal feature extraction and classification. Moreover, the hyperparameters, including the number of convolutional layers, kernel size, and max-pooling size, are explicitly specified, offering a detailed glimpse into the configurational aspects that contribute to the model's efficacy in medical image analysis [Rimal]."
      ],
      "metadata": {
        "id": "XO5Qewj-1VjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Data Preprocessing**"
      ],
      "metadata": {
        "id": "kq9drPCd0SOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import optimizers\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "from PIL import ImageOps\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "WhNMiy2svfkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFutTaHzq4qD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data path for madison\n",
        "#data_dir = \"/content/drive/MyDrive/fall23/dsc201/Dataset_BUSI_with_GT/\""
      ],
      "metadata": {
        "id": "CljVCbEo1Ds0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data path for sofia\n",
        "#data_dir = \"/content/drive/MyDrive/DSC201/Project3/Dataset_BUSI_with_GT/\""
      ],
      "metadata": {
        "id": "pMaqJz4b1SRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Path for Anthony\n",
        "data_dir = \"/content/drive/MyDrive/DSC201/Images/Dataset_BUSI_with_GT/\""
      ],
      "metadata": {
        "id": "K9UWNotwxOQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Path for Dr. Rimal\n",
        "#data_dir = \"/content/drive/MyDrive/Breast_Cancer/\""
      ],
      "metadata": {
        "id": "5AYDNeXVTxSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_resize_image(image_path, desired_width, desired_height):\n",
        "    im = Image.open(image_path)\n",
        "\n",
        "    width, height = im.size\n",
        "\n",
        "    left = 0\n",
        "    right = 2 * width / 3\n",
        "    bottom = 2 * height / 3\n",
        "\n",
        "    im1 = im.crop((left, 0, right, bottom))\n",
        "\n",
        "    im1 = ImageOps.grayscale(im1)\n",
        "\n",
        "    return im1.resize((desired_width, desired_height))"
      ],
      "metadata": {
        "id": "b8VB-kFbUoAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desired_width = 300\n",
        "desired_height = 300\n",
        "\n",
        "image_files = [file for file in os.listdir(data_dir) if file.endswith('.png') and 'mask' in file and 'normal' not in file]\n",
        "labels = [image_file.split('_')[0] for image_file in image_files]\n",
        "\n",
        "label_pattern = re.compile(r'[^0-9()]+')\n",
        "labels = [label_pattern.match(image_file).group() for image_file in image_files]\n",
        "\n",
        "dataset = []\n",
        "\n",
        "for image_file, label in zip(image_files, labels):\n",
        "    image_path = os.path.join(data_dir, image_file)\n",
        "    resized_image = load_and_resize_image(image_path, desired_width, desired_height)\n",
        "    if resized_image is not None:\n",
        "        dataset.append({'image': np.array(resized_image), 'label': label})\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "if not dataset:\n",
        "    raise ValueError(\"Dataset is empty. Please check the loading and resizing of images.\")"
      ],
      "metadata": {
        "id": "HJ2z9_zn5UlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure to display the images\n",
        "fig, axs = plt.subplots(3, 4, figsize=(18, 12))\n",
        "\n",
        "# Display the first twelve images\n",
        "for i in range(3):\n",
        "    for j in range(4):\n",
        "        index = i * 4 + j\n",
        "        if index < len(dataset):\n",
        "            image_data = dataset[index]['image']\n",
        "            label = dataset[index]['label']\n",
        "\n",
        "            # Display the image using imshow\n",
        "            axs[i, j].imshow(image_data)\n",
        "            axs[i, j].set_title(label)\n",
        "            axs[i, j].axis('off')\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8tmIqsXnKbQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Normalization**"
      ],
      "metadata": {
        "id": "xRhf34EXaWpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate images and labels from the dataset\n",
        "grayscale_images = [ImageOps.grayscale(Image.fromarray(data['image'])) for data in dataset]\n",
        "images = np.array([np.array(image) for image in grayscale_images])\n",
        "labels = np.array([data['label'] for data in dataset])\n",
        "\n",
        "# Convert labels to categorical format\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "categorical_labels = to_categorical(encoded_labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(images, categorical_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize pixel values to a range between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "nyGawpd4T1Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "QbjO2P2eXJ9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Building the CNN Model**"
      ],
      "metadata": {
        "id": "ncWmq9-OImZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Build_CNN_Model(conv_layers, kernel_size, max_pool_size, optimizer='Adam', learning_rate=0.001):\n",
        "    model = Sequential()\n",
        "\n",
        "    # 1st Convolutional layer\n",
        "    model.add(Conv2D(conv_layers[0], kernel_size, activation='relu', strides=(6, 6), input_shape=(300, 300, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # 2nd Convolutional layer\n",
        "    model.add(Conv2D(conv_layers[1], (5, 5), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # # 3rd Convolutional layer\n",
        "    # model.add(Conv2D(conv_layers[2], (3, 3), activation='relu', padding='same'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # 4th Convolutional layer\n",
        "    # model.add(Conv2D(conv_layers[3], (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "    # 5th Convolutional layer\n",
        "    # model.add(Conv2D(conv_layers[4], (3, 3), activation='relu', padding='same'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    # model.add(BatchNormalization())\n",
        "\n",
        "    # Flatten layer\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Fully connected layer\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    # model.add(Dropout(0.5))\n",
        "    # model.add(Dense(4, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "    if optimizer == 'Adam':\n",
        "        opt = optimizers.Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'Adagrad':\n",
        "        opt = optimizers.Adagrad(learning_rate=learning_rate)\n",
        "    elif optimizer == 'Nadam':\n",
        "        opt = optimizers.Nadam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'Ndadelta':\n",
        "        opt = optimizers.Adadelta(learning_rate=learning_rate)\n",
        "    elif optimizer == 'Rmsprop':\n",
        "        opt = optimizers.RMSprop(learning_rate=learning_rate)\n",
        "    else:\n",
        "        print(\"No optimizer found in the list(['Adam', 'Adagrad', 'Nadam', 'Adadelta', 'Rmsprop'])! \"\n",
        "              \"Please apply your optimizer manually...\")\n",
        "\n",
        "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    #print(model.summary())\n",
        "    return model"
      ],
      "metadata": {
        "id": "5h1DO_RADDej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_names = ['Adam', 'Adagrad']\n",
        "learning_rate = 0.001\n",
        "conv_layers = [64, 32]\n",
        "kernel_size = (5,5)\n",
        "max_pool_size = (2,2)\n",
        "Build_CNN_Model(conv_layers, kernel_size, max_pool_size, optimizer = optimizer_names[0], learning_rate= learning_rate).summary()"
      ],
      "metadata": {
        "id": "IAxsg4C4zP2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Build_CNN_Model(conv_layers, kernel_size, (2,2), 'Adam',\n",
        "                                            learning_rate=0.01)\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "model.fit(x_train, y_train, batch_size=16, epochs=2, validation_data=(x_test, y_test), callbacks=[callback])"
      ],
      "metadata": {
        "id": "8eEP31SOmw3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "ERkjqpqiIsK5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fca391a"
      },
      "outputs": [],
      "source": [
        "def write_dic_to_file(dic_name, file_name):\n",
        "    with open(file_name, 'w') as file:\n",
        "        file.write(str(dic_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "512c6a90"
      },
      "outputs": [],
      "source": [
        "def CNN_Hyper_Parameter_Tuning(conv_layers, kernel_size, max_pool_shape, optimizers_names,\n",
        "                               learning_rates, batch_sizes, epochs, num_replicates=2):\n",
        "\n",
        "    best_avg_accuracy = 0.0\n",
        "    collect_accuracy = []\n",
        "    all_avg_accuracy = np.zeros((len(optimizers_names), len(learning_rates), len(batch_sizes)))\n",
        "\n",
        "    best_hyper_parameters = {\"model\": conv_layers,\n",
        "                           \"max_pool_shape\": None,\n",
        "                           \"optimizer\": None,\n",
        "                           \"learning_rate\": None,\n",
        "                           \"batch_size\": None,\n",
        "                           \"best_avg_accuracy\": None}\n",
        "\n",
        "    for opt in range(len(optimizers_names)):\n",
        "        for lr in range(len(learning_rates)):\n",
        "            for bs in range(len(batch_sizes)):\n",
        "                for i in range(num_replicates):\n",
        "                    print(\"Running for \" + optimizers_names[opt] + \" optimizer \" + str(learning_rates[lr]) + \\\n",
        "                          \" learning_rate \" + str(batch_sizes[bs]) + \" batch_size and \" + str(i) + \" replicate \" + \"\\n\")\n",
        "\n",
        "                    model = Build_CNN_Model(conv_layers, kernel_size, max_pool_shape, optimizers_names[opt],\n",
        "                                            learning_rate=learning_rates[lr])\n",
        "                    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "                    history = model.fit(x_train, y_train, batch_size=batch_sizes[bs], epochs=epochs,\n",
        "                                        validation_data=(x_test, y_test), callbacks=[callback])\n",
        "                    collect_accuracy.append(history.history['accuracy'])\n",
        "\n",
        "                avg_accuracy = np.mean(np.array(collect_accuracy))\n",
        "                print(\"Average accuracy for this model: \", avg_accuracy)\n",
        "                all_avg_accuracy[opt][lr][bs] = avg_accuracy\n",
        "\n",
        "                if avg_accuracy > best_avg_accuracy:\n",
        "                    best_avg_accuracy = avg_accuracy\n",
        "                    best_hyper_parameters = {\"model\": conv_layers,\n",
        "                                           \"max_pool_shape\": max_pool_shape,\n",
        "                                           \"optimizer\": optimizers_names[opt],\n",
        "                                           \"learning_rate\": learning_rates[lr],\n",
        "                                           \"batch_size\": batch_sizes[bs],\n",
        "                                           \"best_avg_accuracy\": best_avg_accuracy}\n",
        "\n",
        "    output_dictionary = {\n",
        "        \"best_hyper_parameters\": best_hyper_parameters,\n",
        "        \"all_avg_accuracy\": all_avg_accuracy\n",
        "    }\n",
        "\n",
        "    # writing output dictionary to a file\n",
        "    file_name = \"cnn-\" + str(conv_layers[0]) + \"-hyperparameter_tuning_results\" + \".txt\"\n",
        "    write_dic_to_file(output_dictionary, file_name)\n",
        "\n",
        "    print(\"Best_hyper_parameters(CNN): \\n\", output_dictionary['best_hyper_parameters'])\n",
        "    print(\"all_avg_accuracy(CNN): \\n\", output_dictionary['all_avg_accuracy'])\n",
        "\n",
        "    return output_dictionary['best_hyper_parameters']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1380ade"
      },
      "outputs": [],
      "source": [
        "conv_layers = [64, 32]\n",
        "kernel_size = (5, 5)\n",
        "max_pool_size = (4, 4)\n",
        "#kernel_size = (3, 3)\n",
        "#max_pool_size = (2, 2)\n",
        "optimizer_names = ['Adam', 'Nadam']\n",
        "learning_rates = [0.001, 0.01]\n",
        "batch_sizes = [32, 64]\n",
        "epochs = 5\n",
        "num_replicates = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UynLrOkiKIif"
      },
      "outputs": [],
      "source": [
        "alexnet_best_hyper_parameters = CNN_Hyper_Parameter_Tuning(conv_layers, kernel_size, max_pool_size,\n",
        "                                                           optimizer_names, learning_rates, batch_sizes,\n",
        "                                                           epochs=epochs, num_replicates=num_replicates)\n",
        "alexnet_best_hyper_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGj8pPG6GRvT"
      },
      "source": [
        "##**Best CNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3adbf852"
      },
      "outputs": [],
      "source": [
        "def Final_CNN_Model(conv_layers, kernel_size, max_pool_shape, hyper_parameters, epochs=5, num_replicates=10):\n",
        "    # Arrays for collecting performance scores\n",
        "    accuracy_array = np.zeros(num_replicates)\n",
        "    elapsed_time_array = np.zeros(num_replicates)\n",
        "\n",
        "    models_history = []\n",
        "\n",
        "    for i in range(num_replicates):\n",
        "        print(\"Program is running for %d replicate ----->\\n\" % i)\n",
        "\n",
        "        model = Build_CNN_Model(conv_layers, kernel_size, max_pool_shape, optimizer=hyper_parameters[\"optimizer\"],\n",
        "                                    learning_rate=hyper_parameters[\"learning_rate\"])\n",
        "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "        start = time.time()\n",
        "        history = model.fit(x_train, y_train, batch_size=hyper_parameters[\"batch_size\"], epochs=epochs,\n",
        "                            validation_data=(x_test, y_test), callbacks=[callback])\n",
        "        end = time.time()\n",
        "        elapsed_time = end - start\n",
        "\n",
        "        models_history.append(history)\n",
        "\n",
        "        # Calculating performance scores\n",
        "        accuracy_array[i] = history.history['accuracy'][-1]\n",
        "        elapsed_time_array[i] = elapsed_time\n",
        "\n",
        "    avg_accuracy = np.mean(accuracy_array)\n",
        "    avg_elapsed_time = np.mean(elapsed_time_array)\n",
        "\n",
        "    # Collecting important results\n",
        "    performance_metrics = {\n",
        "        'scores': {'accuracy': accuracy_array, 'elapsed_time': elapsed_time_array},\n",
        "        'avg_scores': {'accuracy': avg_accuracy, 'elapsed_time': avg_elapsed_time},\n",
        "        'stds': {'accuracy': np.std(accuracy_array), 'elapsed_time': np.std(elapsed_time_array)},\n",
        "        'maximums': {'accuracy': np.max(accuracy_array), 'elapsed_time': np.max(elapsed_time_array)}\n",
        "    }\n",
        "\n",
        "    model_with_best_accuracy = {\n",
        "        'replicate': np.argmax(accuracy_array),\n",
        "        'accuracy': np.max(accuracy_array),\n",
        "        'elapsed_time': elapsed_time_array[np.argmax(accuracy_array)],\n",
        "        'history': models_history[np.argmax(accuracy_array)].history\n",
        "    }\n",
        "\n",
        "    # Collecting all the outputs together\n",
        "    output_dictionary = {\n",
        "        'best_model': model_with_best_accuracy,\n",
        "        'hyper_parameters': hyper_parameters,\n",
        "        'performance_metrics': performance_metrics,\n",
        "        'models_history': models_history\n",
        "    }\n",
        "\n",
        "    print(\"Progress: All works are done successfully, congratulations!!\\n\")\n",
        "    return output_dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSGMBBW8GWgt"
      },
      "outputs": [],
      "source": [
        "best_hyperparameters = {'model': [64, 32],\n",
        "                        'max_pool_shape': (4, 4),\n",
        "                        'optimizer': 'Adam',\n",
        "                        'learning_rate': 0.001,\n",
        "                        'batch_size': 64,\n",
        "                        'best_avg_accuracy': 0.7302443587779999}\n",
        "\n",
        "epochs = 10\n",
        "num_replicates = 20\n",
        "\n",
        "cnn_output = Final_CNN_Model(conv_layers=best_hyperparameters['model'],\n",
        "                                      kernel_size=(5, 5),\n",
        "                                      max_pool_shape=best_hyperparameters['max_pool_shape'],\n",
        "                                      hyper_parameters=best_hyperparameters,\n",
        "                                      epochs=epochs,\n",
        "                                      num_replicates=num_replicates)\n",
        "\n",
        "cnn_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Confusion Matrix**"
      ],
      "metadata": {
        "id": "Hvmpe66bil9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = cnn_output['best_model']['history']\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "class_labels = label_encoder.classes_\n",
        "class_mapping = {i: class_labels[i] for i in range(len(class_labels))}\n",
        "y_true_labels = [class_mapping[label] for label in y_true]\n",
        "y_pred_labels = [class_mapping[label] for label in y_pred]\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true_labels, y_pred_labels, labels=class_labels)\n",
        "\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JYKptrx5qOr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_report = classification_report(y_true_labels, y_pred_labels, target_names=class_labels)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "id": "5kiNrJM2jveT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OmPzPaXG1VQf",
        "kq9drPCd0SOB",
        "xRhf34EXaWpM",
        "ncWmq9-OImZb",
        "ERkjqpqiIsK5",
        "hGj8pPG6GRvT",
        "Hvmpe66bil9w"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}